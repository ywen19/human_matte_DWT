#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import subprocess
import torch
import numpy as np

def check_cuda_environment():
    """æ£€æŸ¥CUDAç¯å¢ƒ"""
    print("=" * 60)
    print("CUDAç¯å¢ƒè¯Šæ–­")
    print("=" * 60)
    
    # 1. ç³»ç»ŸCUDAç‰ˆæœ¬
    try:
        result = subprocess.run(['nvcc', '--version'], capture_output=True, text=True)
        if result.returncode == 0:
            print("ç³»ç»ŸCUDAç‰ˆæœ¬:")
            for line in result.stdout.split('\n'):
                if 'release' in line.lower():
                    print(f"  {line.strip()}")
        else:
            print("âŒ æ— æ³•è·å–ç³»ç»ŸCUDAç‰ˆæœ¬")
    except FileNotFoundError:
        print("âŒ nvccæœªæ‰¾åˆ°ï¼ŒCUDAå¯èƒ½æœªæ­£ç¡®å®‰è£…")
    
    # 2. PyTorch CUDAç‰ˆæœ¬
    print(f"\nPyTorchä¿¡æ¯:")
    print(f"  PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"  CUDAå¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"  PyTorch CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print(f"  GPUè®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        
        # æ˜¾ç¤ºæ‰€æœ‰GPUä¿¡æ¯
        for i in range(torch.cuda.device_count()):
            print(f"  GPU {i}: {torch.cuda.get_device_name(i)}")
            props = torch.cuda.get_device_properties(i)
            print(f"    å†…å­˜: {props.total_memory / 1024**3:.1f} GB")
            print(f"    è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
    
    # 3. NumPyä¿¡æ¯
    print(f"\nNumPyä¿¡æ¯:")
    print(f"  NumPyç‰ˆæœ¬: {np.__version__}")
    
    # 4. ç¯å¢ƒå˜é‡
    print(f"\nç¯å¢ƒå˜é‡:")
    cuda_home = os.environ.get('CUDA_HOME', 'æœªè®¾ç½®')
    print(f"  CUDA_HOME: {cuda_home}")
    ld_path = os.environ.get('LD_LIBRARY_PATH', 'æœªè®¾ç½®')
    print(f"  LD_LIBRARY_PATH: {'å·²è®¾ç½®' if ld_path != 'æœªè®¾ç½®' else 'æœªè®¾ç½®'}")

def check_package_versions():
    """æ£€æŸ¥å…³é”®åŒ…ç‰ˆæœ¬"""
    print("\n" + "=" * 60)
    print("å…³é”®åŒ…ç‰ˆæœ¬æ£€æŸ¥")
    print("=" * 60)
    
    # æ›´æ–°åŒ…åˆ—è¡¨ï¼ŒåŒ¹é…æ‚¨çš„ç¯å¢ƒ
    packages = [
        'torch', 'torchvision', 'torchaudio', 'numpy', 'ultralytics', 
        'opencv-python', 'pillow', 'matplotlib', 'hydra-core', 'omegaconf'
    ]
    
    for package in packages:
        try:
            result = subprocess.run([sys.executable, '-m', 'pip', 'show', package], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                lines = result.stdout.split('\n')
                name = next((line.split(': ')[1] for line in lines if line.startswith('Name:')), 'Unknown')
                version = next((line.split(': ')[1] for line in lines if line.startswith('Version:')), 'Unknown')
                print(f"  {name}: {version}")
            else:
                print(f"  {package}: æœªå®‰è£…")
        except Exception as e:
            print(f"  {package}: æ£€æŸ¥å¤±è´¥ - {e}")

def test_cuda_compatibility():
    """æµ‹è¯•CUDAå…¼å®¹æ€§"""
    print("\n" + "=" * 60)
    print("CUDAå…¼å®¹æ€§æµ‹è¯•")
    print("=" * 60)
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨")
        return False
    
    try:
        # 1. åŸºæœ¬CUDAæ“ä½œ
        print("1. æµ‹è¯•åŸºæœ¬CUDAæ“ä½œ...")
        a = torch.randn(1000, 1000).cuda()
        b = torch.randn(1000, 1000).cuda()
        c = torch.mm(a, b)
        print("  âœ… åŸºæœ¬CUDAæ“ä½œæˆåŠŸ")
        
        # 2. å¤šGPUæµ‹è¯•ï¼ˆå¦‚æœæœ‰å¤šä¸ªGPUï¼‰
        if torch.cuda.device_count() > 1:
            print("2. æµ‹è¯•å¤šGPUæ“ä½œ...")
            device_0 = torch.device('cuda:0')
            device_1 = torch.device('cuda:1')
            x = torch.randn(100, 100, device=device_0)
            y = torch.randn(100, 100, device=device_1)
            print(f"  âœ… å¤šGPUæ“ä½œæˆåŠŸ (å…±{torch.cuda.device_count()}ä¸ªGPU)")
        
        # 3. CPU-GPUæ•°æ®ä¼ è¾“
        print("3. æµ‹è¯•CPU-GPUæ•°æ®ä¼ è¾“...")
        cpu_tensor = torch.randn(1000, 1000)
        gpu_tensor = cpu_tensor.cuda()
        back_to_cpu = gpu_tensor.cpu()
        print("  âœ… CPU-GPUæ•°æ®ä¼ è¾“æˆåŠŸ")
        
        # 4. NumPy-PyTorchäº’è½¬ï¼ˆæ‚¨çš„ç¯å¢ƒä¸­çš„å…³é”®æµ‹è¯•ï¼‰
        print("4. æµ‹è¯•NumPy-PyTorchäº’è½¬...")
        np_array = np.random.randn(100, 100).astype(np.float32)
        torch_tensor = torch.from_numpy(np_array)
        back_to_numpy = torch_tensor.numpy()
        
        # æµ‹è¯•CUDAå¼ é‡è½¬æ¢
        cuda_tensor = torch_tensor.cuda()
        cpu_numpy = cuda_tensor.cpu().numpy()
        print("  âœ… NumPy-PyTorchäº’è½¬æˆåŠŸ")
        
        # 5. CUDAå†…å­˜ç®¡ç†
        print("5. æµ‹è¯•CUDAå†…å­˜ç®¡ç†...")
        torch.cuda.empty_cache()
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / 1024**2
            reserved = torch.cuda.memory_reserved(i) / 1024**2
            print(f"  GPU {i} å†…å­˜: å·²åˆ†é… {allocated:.1f} MB, å·²ä¿ç•™ {reserved:.1f} MB")
        print("  âœ… CUDAå†…å­˜ç®¡ç†æ­£å¸¸")
        
        return True
        
    except Exception as e:
        print(f"âŒ CUDAå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_sam2_yolo_environment():
    """æµ‹è¯•SAM2+YOLOç¯å¢ƒï¼ˆä»preprocessç›®å½•è¿è¡Œï¼‰"""
    print("\n" + "=" * 60)
    print("SAM2 + YOLO ç¯å¢ƒæµ‹è¯• (ä»preprocessç›®å½•)")
    print("=" * 60)
    
    # 1. æµ‹è¯•YOLO
    print("1. æµ‹è¯•YOLO...")
    try:
        # åœ¨preprocessç›®å½•ä¸­ï¼ŒYOLOæ¨¡å‹å°±åœ¨å½“å‰ç›®å½•
        yolo_path = "./yolov8n.pt"  # å½“å‰ç›®å½•ä¸‹
        if os.path.exists(yolo_path):
            from ultralytics import YOLO
            model = YOLO(yolo_path)
            print(f"  âœ… YOLOæ¨¡å‹åŠ è½½æˆåŠŸ: {yolo_path}")
            
            # å¿«é€Ÿæ¨ç†æµ‹è¯•
            test_img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
            results = model(test_img, verbose=False)
            print(f"  âœ… YOLOæ¨ç†æˆåŠŸ")
        else:
            print(f"  âŒ YOLOæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {yolo_path}")
    except Exception as e:
        print(f"  âŒ YOLOæµ‹è¯•å¤±è´¥: {e}")
    
    # 2. æµ‹è¯•SAM2ç¯å¢ƒ
    print("2. æµ‹è¯•SAM2ç¯å¢ƒ...")
    try:
        # ä»preprocessç›®å½•ï¼ŒSAM2è·¯å¾„æ˜¯ ../sam2
        sam2_path = "../sam2"
        config_path = "../sam2/sam2/configs/sam2_hiera_l.yaml"
        model_path = "../local_sam2_hiera_large/sam2_hiera_large.pt"
        
        if os.path.exists(sam2_path):
            print(f"  âœ… SAM2ä»£ç ç›®å½•å­˜åœ¨: {sam2_path}")
        else:
            print(f"  âŒ SAM2ä»£ç ç›®å½•ä¸å­˜åœ¨: {sam2_path}")
            
        if os.path.exists(config_path):
            print(f"  âœ… SAM2é…ç½®æ–‡ä»¶å­˜åœ¨: {config_path}")
        else:
            print(f"  âŒ SAM2é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            
        if os.path.exists(model_path):
            size = os.path.getsize(model_path) / 1024**2
            print(f"  âœ… SAM2æ¨¡å‹æ–‡ä»¶å­˜åœ¨: {model_path} ({size:.1f} MB)")
        else:
            print(f"  âŒ SAM2æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            
        # å°è¯•å¯¼å…¥SAM2ï¼ˆå¦‚æœè·¯å¾„æ­£ç¡®ï¼‰
        if os.path.exists(sam2_path):
            sys.path.append(os.path.abspath(sam2_path))
            try:
                from sam2.sam2_image_predictor import SAM2ImagePredictor
                print(f"  âœ… SAM2æ¨¡å—å¯¼å…¥æˆåŠŸ")
            except Exception as e:
                print(f"  âš ï¸ SAM2æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
                
    except Exception as e:
        print(f"  âŒ SAM2ç¯å¢ƒæ£€æŸ¥å¤±è´¥: {e}")
    
    # 3. æ˜¾ç¤ºå½“å‰ç›®å½•ä¿¡æ¯
    print("3. å½“å‰ç›®å½•ç»“æ„æ£€æŸ¥...")
    current_dir = os.getcwd()
    print(f"  å½“å‰å·¥ä½œç›®å½•: {current_dir}")
    
    # åˆ—å‡ºå½“å‰ç›®å½•çš„æ–‡ä»¶
    files = [f for f in os.listdir('.') if f.endswith(('.pt', '.py'))]
    if files:
        print(f"  å½“å‰ç›®å½•æ–‡ä»¶:")
        for f in files:
            print(f"    - {f}")
    
    # æ£€æŸ¥çˆ¶ç›®å½•
    parent_files = []
    try:
        parent_files = [f for f in os.listdir('..') if os.path.isdir(os.path.join('..', f))]
        print(f"  çˆ¶ç›®å½•åŒ…å«: {', '.join(parent_files[:5])}{'...' if len(parent_files) > 5 else ''}")
    except:
        pass

def suggest_fixes():
    """å»ºè®®ä¿®å¤æ–¹æ¡ˆ"""
    print("\n" + "=" * 60)
    print("ä¼˜åŒ–å»ºè®®")
    print("=" * 60)
    
    # æ£€æŸ¥PyTorchå’ŒCUDAç‰ˆæœ¬åŒ¹é…
    pytorch_version = torch.__version__
    cuda_version = torch.version.cuda if torch.cuda.is_available() else "N/A"
    
    print(f"å½“å‰é…ç½®: PyTorch {pytorch_version} + CUDA {cuda_version}")
    
    # æ ¹æ®æ‚¨çš„æˆåŠŸé…ç½®ç»™å‡ºå»ºè®®
    if "+cu118" in pytorch_version and "1.26" in np.__version__:
        print("\nğŸ‰ å½“å‰é…ç½®çœ‹èµ·æ¥å¾ˆå¥½ï¼")
        print("âœ… PyTorch 2.0.1+cu118 - æ­£ç¡®")
        print("âœ… NumPy 1.26.x - å…¼å®¹")
        print("âœ… CUDA 11.8 æ”¯æŒ")
        
        print("\nğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
        print("1. ç¡®ä¿è®¾ç½®äº†æ­£ç¡®çš„ç¯å¢ƒå˜é‡:")
        print("   export CUDA_HOME=/home/wy/anaconda3/envs/cuda118")
        print("   export PATH=$CUDA_HOME/bin:$PATH")
        print("   export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$CUDA_HOME/lib:$LD_LIBRARY_PATH")
        
        print("\n2. å¦‚æœé‡åˆ°ä»»ä½•é—®é¢˜ï¼Œé‡å¯ç¯å¢ƒ:")
        print("   conda deactivate && conda activate sam2_yolo")
        
    else:
        print("\nğŸ”§ ä¿®å¤æ–¹æ¡ˆ (æŒ‰ä¼˜å…ˆçº§æ’åº):")
        
        print("\n1. æ¢å¤åˆ°æˆåŠŸé…ç½®:")
        print("   pip uninstall torch torchvision torchaudio")
        print("   pip install torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 \\")
        print("       --index-url https://download.pytorch.org/whl/cu118")
        
        print("\n2. æ¢å¤NumPyç‰ˆæœ¬:")
        print("   pip install numpy==1.26.4 --force-reinstall")

def create_test_script():
    """åˆ›å»ºé’ˆå¯¹preprocessç›®å½•çš„æµ‹è¯•è„šæœ¬"""
    print("\n" + "=" * 60)
    print("åˆ›å»ºç¯å¢ƒæµ‹è¯•è„šæœ¬ (preprocessç›®å½•ç‰ˆæœ¬)")
    print("=" * 60)
    
    test_script = '''#!/usr/bin/env python3
"""
SAM2 + YOLO ç¯å¢ƒå¿«é€Ÿæµ‹è¯•è„šæœ¬
ä¸“ä¸ºpreprocessç›®å½•è¿è¡Œè®¾è®¡
"""

import torch
import numpy as np
import sys
import os

def quick_test():
    """å¿«é€Ÿç¯å¢ƒæµ‹è¯• (ä»preprocessç›®å½•)"""
    print("ğŸš€ å¼€å§‹å¿«é€Ÿç¯å¢ƒæµ‹è¯•... (ä»preprocessç›®å½•)")
    print(f"ğŸ“ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
    
    # 1. åŸºç¡€ç¯å¢ƒæ£€æŸ¥
    print(f"\\nâœ… Python: {sys.version.split()[0]}")
    print(f"âœ… PyTorch: {torch.__version__}")
    print(f"âœ… NumPy: {np.__version__}")
    print(f"âœ… CUDAå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"âœ… GPUæ•°é‡: {torch.cuda.device_count()}")
    
    # 2. YOLOæµ‹è¯• (å½“å‰ç›®å½•)
    print("\\nğŸ“‹ æµ‹è¯•YOLO...")
    try:
        from ultralytics import YOLO
        # åœ¨preprocessç›®å½•ä¸­ï¼Œæ¨¡å‹å°±åœ¨å½“å‰ç›®å½•
        model = YOLO("./yolov8n.pt")
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        test_img = np.random.randint(0, 255, (640, 640, 3), dtype=np.uint8)
        results = model(test_img, verbose=False)
        print("âœ… YOLOæµ‹è¯•é€šè¿‡")
    except Exception as e:
        print(f"âŒ YOLOæµ‹è¯•å¤±è´¥: {e}")
    
    # 3. SAM2æµ‹è¯• (çˆ¶ç›®å½•)
    print("\\nğŸ“‹ æµ‹è¯•SAM2...")
    try:
        # ä»preprocessç›®å½•ï¼ŒSAM2åœ¨çˆ¶ç›®å½•
        sys.path.append(os.path.abspath("../sam2"))
        
        from hydra import initialize_config_dir, compose
        from hydra.core.global_hydra import GlobalHydra
        from hydra.utils import instantiate
        from omegaconf import OmegaConf
        from sam2.sam2_image_predictor import SAM2ImagePredictor
        
        # æ¸…é™¤Hydraå®ä¾‹
        if GlobalHydra().is_initialized():
            GlobalHydra.instance().clear()
        
        # åŠ è½½SAM2
        config_dir = os.path.abspath("../sam2/sam2/configs")
        with initialize_config_dir(config_dir=config_dir, version_base=None):
            cfg = compose(config_name="sam2_hiera_l.yaml")
            OmegaConf.resolve(cfg)
            
            device = "cuda" if torch.cuda.is_available() else "cpu"
            model = instantiate(cfg.model, _recursive_=True)
            
            # åŠ è½½æƒé‡
            model_path = "../local_sam2_hiera_large/sam2_hiera_large.pt"
            checkpoint = torch.load(model_path, map_location="cpu", weights_only=True)
            model.load_state_dict(checkpoint["model"])
            
            model = model.to(device).eval()
            predictor = SAM2ImagePredictor(model)
            print("âœ… SAM2æµ‹è¯•é€šè¿‡")
            
    except Exception as e:
        print(f"âŒ SAM2æµ‹è¯•å¤±è´¥: {e}")
    
    # 4. é›†æˆæµ‹è¯•
    print("\\nğŸ“‹ é›†æˆæµ‹è¯•...")
    try:
        # ä½¿ç”¨ç›¸åŒçš„æµ‹è¯•å›¾åƒ
        test_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
        
        # YOLOæ£€æµ‹
        yolo_model = YOLO("./yolov8n.pt")
        yolo_results = yolo_model(test_image, verbose=False)
        detections = yolo_results[0].boxes
        num_detections = len(detections) if detections is not None else 0
        
        # SAM2åˆ†å‰²
        predictor.set_image(test_image)
        input_point = np.array([[320, 240]])
        input_label = np.array([1])
        masks, scores, logits = predictor.predict(
            point_coords=input_point,
            point_labels=input_label,
            multimask_output=True,
        )
        
        print(f"âœ… é›†æˆæµ‹è¯•æˆåŠŸ!")
        print(f"   YOLOæ£€æµ‹: {num_detections} ä¸ªå¯¹è±¡")
        print(f"   SAM2åˆ†å‰²: {len(masks)} ä¸ªmaskï¼Œæœ€ä½³åˆ†æ•°: {scores.max():.3f}")
        
    except Exception as e:
        print(f"âŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
    
    print("\\nğŸ‰ ç¯å¢ƒæµ‹è¯•å®Œæˆ!")

if __name__ == "__main__":
    quick_test()
'''
    
    with open('quick_env_test_preprocess.py', 'w') as f:
        f.write(test_script)
    
    print("âœ… å·²åˆ›å»º quick_env_test_preprocess.py")
    print("é€‚åˆåœ¨preprocessç›®å½•è¿è¡Œ: python quick_env_test_preprocess.py")

def main():
    print("ğŸ” SAM2 + YOLO ç¯å¢ƒè¯Šæ–­å¼€å§‹...\n")
    
    # 1. æ£€æŸ¥CUDAç¯å¢ƒ
    check_cuda_environment()
    
    # 2. æ£€æŸ¥åŒ…ç‰ˆæœ¬
    check_package_versions()
    
    # 3. æµ‹è¯•CUDAå…¼å®¹æ€§
    cuda_ok = test_cuda_compatibility()
    
    # 4. æµ‹è¯•SAM2+YOLOç¯å¢ƒï¼ˆæ–°å¢ï¼‰
    test_sam2_yolo_environment()
    
    # 5. æä¾›ä¿®å¤å»ºè®®
    suggest_fixes()
    
    # 6. åˆ›å»ºæµ‹è¯•è„šæœ¬
    create_test_script()
    
    print(f"\n{'='*60}")
    print("è¯Šæ–­å®Œæˆ!")
    if cuda_ok:
        print("âœ… CUDAç¯å¢ƒæ­£å¸¸")
    else:
        print("âš ï¸  æ£€æµ‹åˆ°CUDAå…¼å®¹æ€§é—®é¢˜")
    print(f"{'='*60}")

if __name__ == "__main__":
    main()